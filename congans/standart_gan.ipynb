{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (60000, 28, 28) (60000,)\n",
      "Test (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# example of loading the fashion_mnist dataset\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "# load the images into memory\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "# summarize the shape of the dataset\n",
    "print('Train', trainX.shape, trainy.shape)\n",
    "print('Test', testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_plot(examples, n):\n",
    "\t# plot images\n",
    "\tfor i in range(n * n):\n",
    "\t\t# define subplot\n",
    "\t\tplt.subplot(n, n, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tplt.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tplt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "\tmodel = tf.keras.Sequential()\n",
    "\t# downsample\n",
    "\tmodel.add(tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape))\n",
    "\tmodel.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\t# downsample\n",
    "\tmodel.add(tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\t# classifier\n",
    "\tmodel.add(tf.keras.layers.Flatten())\n",
    "\tmodel.add(tf.keras.layers.Dropout(0.4))\n",
    "\tmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "\tmodel = tf.keras.Sequential()\n",
    "\t# foundation for 7x7 image\n",
    "\tn_nodes = 128 * 7 * 7\n",
    "\tmodel.add(tf.keras.layers.Dense(n_nodes, input_dim=latent_dim))\n",
    "\tmodel.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(tf.keras.layers.Reshape((7, 7, 128)))\n",
    "\t# upsample to 14x14\n",
    "\tmodel.add(tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 28x28\n",
    "\tmodel.add(tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\t# generate\n",
    "\tmodel.add(tf.keras.layers.Conv2D(1, (7,7), activation='tanh', padding='same'))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = tf.keras.Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\topt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load fashion mnist images\n",
    "def load_real_samples():\n",
    "\t# load dataset\n",
    "\t(trainX, _), (_, _) = load_data()\n",
    "\t# expand to 3d, e.g. add channels\n",
    "\tX = np.expand_dims(trainX, axis=-1)\n",
    "\t# convert from ints to floats\n",
    "\tX.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX = (X - 127.5) / 127.5\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# select images\n",
    "\tX = dataset[ix]\n",
    "\t# generate class labels\n",
    "\ty = tf.ones((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=1024):\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# enumerate batches over the training set\n",
    "\t\tfor j in range(bat_per_epo):\n",
    "\t\t\t# get randomly selected 'real' samples\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# update discriminator model weights\n",
    "\t\t\td_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "\t\t\t# generate 'fake' examples\n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# update discriminator model weights\n",
    "\t\t\td_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t\t# prepare points in latent space as input for the generator\n",
    "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t\t# create inverted labels for the fake samples\n",
    "\t\t\ty_gan = np.ones((n_batch, 1))\n",
    "\t\t\t# update the generator via the discriminator's error\n",
    "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\t\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "\t\t\t(i+1,  d_loss1, d_loss2, g_loss))\n",
    "\t# save the generator model\n",
    "\tg_model.save('generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1=0.301, d2=0.869 g=0.637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgan_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlatent_dim\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001B[0m\n\u001B[1;32m     19\u001B[0m \ty_gan \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((n_batch, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     20\u001B[0m \t\u001B[38;5;66;03m# update the generator via the discriminator's error\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \tg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mgan_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_gan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_gan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \t\u001B[38;5;66;03m# summarize loss on this batch\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, d1=\u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m, d2=\u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m g=\u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m     24\u001B[0m \t(i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m,  d_loss1, d_loss2, g_loss))\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1730\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001B[0m\n\u001B[1;32m   1727\u001B[0m   logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[1;32m   1729\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reset_metrics:\n\u001B[0;32m-> 1730\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1731\u001B[0m logs \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39mto_numpy_or_python_type(logs)\n\u001B[1;32m   1732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1669\u001B[0m, in \u001B[0;36mModel.reset_metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1650\u001B[0m \u001B[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001B[39;00m\n\u001B[1;32m   1651\u001B[0m \n\u001B[1;32m   1652\u001B[0m \u001B[38;5;124;03mExamples:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1666\u001B[0m \n\u001B[1;32m   1667\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1668\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics:\n\u001B[0;32m-> 1669\u001B[0m   \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_states\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/keras/metrics.py:253\u001B[0m, in \u001B[0;36mMetric.reset_states\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset_states\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    248\u001B[0m   \u001B[38;5;124;03m\"\"\"Resets all of the metric state variables.\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03m  This function is called between epochs/steps,\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03m  when a metric is evaluated during training.\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 253\u001B[0m   \u001B[43mK\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_set_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvariables\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 201\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m    203\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[1;32m    204\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[1;32m    205\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/keras/backend.py:3706\u001B[0m, in \u001B[0;36mbatch_set_value\u001B[0;34m(tuples)\u001B[0m\n\u001B[1;32m   3704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mexecuting_eagerly_outside_functions():\n\u001B[1;32m   3705\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m x, value \u001B[38;5;129;01min\u001B[39;00m tuples:\n\u001B[0;32m-> 3706\u001B[0m     \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3707\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3708\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m get_graph()\u001B[38;5;241m.\u001B[39mas_default():\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:892\u001B[0m, in \u001B[0;36mBaseResourceVariable.assign\u001B[0;34m(self, value, use_locking, name, read_value)\u001B[0m\n\u001B[1;32m    887\u001B[0m     tensor_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m    888\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    889\u001B[0m       (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot assign to variable\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m due to variable shape \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m and value \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    890\u001B[0m        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m are incompatible\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m    891\u001B[0m       (tensor_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape, value_tensor\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m--> 892\u001B[0m assign_op \u001B[38;5;241m=\u001B[39m \u001B[43mgen_resource_variable_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign_variable_op\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read_value:\n\u001B[1;32m    895\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lazy_read(assign_op)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.9/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:141\u001B[0m, in \u001B[0;36massign_variable_op\u001B[0;34m(resource, value, name)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m    140\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 141\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAssignVariableOp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m    144\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m latent_points \u001B[38;5;241m=\u001B[39m generate_latent_points(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m      3\u001B[0m X \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(latent_points)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mshow_plot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [40]\u001B[0m, in \u001B[0;36mshow_plot\u001B[0;34m(examples, n)\u001B[0m\n\u001B[1;32m      7\u001B[0m \tplt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m \t\u001B[38;5;66;03m# plot raw pixel data\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \tplt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mexamples\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgray_r\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mIndexError\u001B[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 6 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.load_model('generator.h5', compile = False)\n",
    "latent_points = generate_latent_points(100, 5)\n",
    "X = model.predict(latent_points)\n",
    "show_plot(X, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}