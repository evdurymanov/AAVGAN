{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from absl import flags\n",
    "import common.model.utils_ori as utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from common.bio.amino_acid import numpy_seqs_to_fasta\n",
    "\n",
    "from common.bio.constants import get_lesk_color_mapping\n",
    "from gan.documentation import add_image_grid\n",
    "from gan.protein.custom_scalars import add_custom_scalar\n",
    "from gan.protein.helpers import convert_to_acid_ids, REAL_PROTEINS, ACID_EMBEDDINGS_SCOPE, ACID_EMBEDDINGS, \\\n",
    "    FAKE_PROTEINS, CLASS_MAPPING, SEQ_LENGTH, get_shape, LABELS, NUM_AMINO_ACIDS, get_file\n",
    "from common.model import ops\n",
    "from common.model.ops import pad_up_to, gelu\n",
    "\n",
    "\n",
    "from gan.parameters import get_flags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def extract_seq_and_label(record, args):\n",
    "    \"\"\"Extracts and preprocesses the sequences and label from the record.\n",
    "\n",
    "    Args:\n",
    "      record: tfrecord from file\n",
    "      args: list of arguments\n",
    "\n",
    "    Returns:\n",
    "      Sequence and label as tensors\n",
    "\n",
    "    \"\"\"\n",
    "    features = tf.parse_single_example(\n",
    "        serialized=record,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([1], tf.int64),\n",
    "            'sequence': tf.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True)\n",
    "        },\n",
    "    )\n",
    "    seq = tf.cast(features['sequence'], tf.int32, name=\"seq\")\n",
    "    labels = tf.cast(features['label'], tf.int32, name=\"labels\")\n",
    "\n",
    "    seq = pad_up_to(seq, args[0], dynamic_padding=args[1])\n",
    "    return seq, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_upsampling_factor(full_path):\n",
    "    \"\"\"\n",
    "    Function that parses the file name to know how much upsampling is required.\n",
    "    Args:\n",
    "        full_path: a path of the tfrecords file\n",
    "\n",
    "    Returns:\n",
    "        return upsampling factor. If file is not in the right format return -1 (infinite upsampling)\n",
    "    \"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(full_path))[0]\n",
    "    parts = filename.split(\"_\")\n",
    "    if len(parts) == 3 or len(parts) == 2:\n",
    "        return int(parts[-1])*int(parts[-2])\n",
    "    else:\n",
    "        return -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_batches(fn, data_dir, batch_size, shuffle_buffer_size=100000, running_mode='train', args=None,\n",
    "                balance=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      fn: function that tells how to process tfrecord\n",
    "      data_dir: The directory to read data from.\n",
    "      batch_size: The number of elements in a single minibatch.\n",
    "      cycle_length: The number of input elements to process concurrently in the dataset loader. (Default: 1)\n",
    "      shuffle_buffer_size: The number of records to load before shuffling. (Default: 100000)\n",
    "      running_mode: string that is used to determine from where the data should be loaded (Default: train)\n",
    "      args: list of arguments that will be passed into function that processes tfrecord (Default: None)\n",
    "\n",
    "    Returns:\n",
    "      A batch worth of data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading files from {}\".format(data_dir))\n",
    "    filenames = tf.gfile.Glob(os.path.join(data_dir, running_mode, \"*.tfrecords\"))\n",
    "    print(\"Found {} file(s)\".format(len(filenames)))\n",
    "    upsampling_factor = [ get_upsampling_factor(fn) for fn in filenames]\n",
    "    print(upsampling_factor)\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices((filenames, upsampling_factor))\n",
    "    print(filename_dataset)\n",
    "    filename_dataset = filename_dataset.shuffle(len(filenames))\n",
    "    prefetch = max(int(batch_size / len(filenames)), 1)\n",
    "\n",
    "    # Repeat data in the file for unlimited number. This solves class imbalance problem.\n",
    "    def get_tfrecord_dataset(filename, upsampling_factor):\n",
    "        tfrecord_dataset = tf.data.TFRecordDataset(filename).prefetch(prefetch)\n",
    "        if balance:\n",
    "            tfrecord_dataset = tfrecord_dataset.repeat(tf.cast(upsampling_factor, dtype=tf.int64))\n",
    "        return tfrecord_dataset\n",
    "\n",
    "    dataset = filename_dataset.interleave(\n",
    "        lambda filename, upsampling_factor: get_tfrecord_dataset(filename, upsampling_factor),\n",
    "        cycle_length=len(filenames))\n",
    "    print(\"Loading process will use {} CPUs\".format(multiprocessing.cpu_count()))\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(lambda x: fn(x, args), num_parallel_calls=multiprocessing.cpu_count()).prefetch(batch_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_batch(batch_size, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding):\n",
    "    path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep))\n",
    "    extract_fn = extract_seq_and_label\n",
    "    batches = get_batches(extract_fn, path, batch_size, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                running_mode=running_mode, args=[[512], dynamic_padding])\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def prepare_real_data(real_x, labels, data_dir, dataset):\n",
    "    real_x = tf.reshape(real_x, [10, 512], name=REAL_PROTEINS)\n",
    "    #reactions = get_reactions(labels, data_dir, dataset)\n",
    "    labels = tf.identity(tf.squeeze(labels), name=LABELS)\n",
    "    real_x = convert_real_to_one_hot(real_x)\n",
    "    real_x = tf.expand_dims(real_x, 1)\n",
    "    return real_x, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_reactions(labels, data_dir, dataset):\n",
    "    reaction_tensors = []\n",
    "    for reaction in get_reaction_data(data_dir, dataset):\n",
    "        # label = tf.convert_to_tensor(reaction[0])\n",
    "        r = [pad_up_to(tf.convert_to_tensor(reaction[i]), [128], True) for\n",
    "             i in range(1, 5)]\n",
    "        reaction_tensors.append(r)\n",
    "    reaction_tensors = tf.stack(reaction_tensors)\n",
    "    return tf.gather_nd(reaction_tensors, tf.expand_dims(labels, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def convert_real_to_one_hot(real_x): # Label smoothing?\n",
    "    real_to_display = tf.one_hot(real_x, 21, axis=1)\n",
    "    real_to_display = tf.transpose(real_to_display, perm=[0, 2, 1])\n",
    "    return real_to_display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_reaction_data(data_dir, dataset):\n",
    "    filename = \"train\" + \"_reactions.npy\"\n",
    "    reaction_path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep), filename)\n",
    "    try:\n",
    "        reactions = np.load(reaction_path)\n",
    "    except Exception as e:\n",
    "        tf.logging.warn(\"Reaction file could not be loaded: \" + e.__str__())\n",
    "        reactions = []\n",
    "    return reactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = \"/home/evgeny/code/AAVGAN/src/gan\"\n",
    "DATASET = 'protein'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from /home/evgeny/code/AAVGAN/src/gan/../../data/protein/capsids\n",
      "Found 1 file(s)\n",
      "[1]\n",
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int32)>\n",
      "Loading process will use 12 CPUs\n",
      "Tensor(\"labels_2:0\", shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(CURRENT_DIRECTORY, '..\\..\\data'.replace(\"\\\\\", os.sep))\n",
    "dataset = DATASET + \"/capsids\"\n",
    "already_embedded = False\n",
    "shuffle_buffer_size = 100000\n",
    "running_mode = \"train\"\n",
    "dynamic_padding = True\n",
    "sess = tf.InteractiveSession()\n",
    "batch = get_batch(10, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding)\n",
    "real_x, labels = batch[0], batch[1]\n",
    "#print(real_x.eval())\n",
    "#print(labels.eval())\n",
    "real_x, labels = prepare_real_data(real_x, labels, data_dir, dataset)\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}