{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from absl import flags\n",
    "import common.model.utils_ori as utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from common.bio.amino_acid import numpy_seqs_to_fasta\n",
    "\n",
    "from common.bio.constants import get_lesk_color_mapping\n",
    "from gan.documentation import add_image_grid\n",
    "from gan.protein.custom_scalars import add_custom_scalar\n",
    "from gan.protein.helpers import convert_to_acid_ids, REAL_PROTEINS, ACID_EMBEDDINGS_SCOPE, ACID_EMBEDDINGS, \\\n",
    "    FAKE_PROTEINS, CLASS_MAPPING, SEQ_LENGTH, get_shape, LABELS, NUM_AMINO_ACIDS, get_file\n",
    "from common.model import ops\n",
    "from common.model.ops import pad_up_to, gelu\n",
    "\n",
    "\n",
    "from gan.parameters import get_flags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "def extract_seq_and_label(record, args):\n",
    "    \"\"\"Extracts and preprocesses the sequences and label from the record.\n",
    "\n",
    "    Args:\n",
    "      record: tfrecord from file\n",
    "      args: list of arguments\n",
    "\n",
    "    Returns:\n",
    "      Sequence and label as tensors\n",
    "\n",
    "    \"\"\"\n",
    "    features = tf.parse_single_example(\n",
    "        serialized=record,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([1], tf.int64),\n",
    "            'sequence': tf.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True)\n",
    "        },\n",
    "    )\n",
    "    seq = tf.cast(features['sequence'], tf.int32, name=\"seq\")\n",
    "    labels = tf.cast(features['label'], tf.int32, name=\"labels\")\n",
    "\n",
    "    seq = pad_up_to(seq, args[0], dynamic_padding=args[1])\n",
    "    return seq, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "def get_upsampling_factor(full_path):\n",
    "    \"\"\"\n",
    "    Function that parses the file name to know how much upsampling is required.\n",
    "    Args:\n",
    "        full_path: a path of the tfrecords file\n",
    "\n",
    "    Returns:\n",
    "        return upsampling factor. If file is not in the right format return -1 (infinite upsampling)\n",
    "    \"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(full_path))[0]\n",
    "    parts = filename.split(\"_\")\n",
    "    if len(parts) == 3 or len(parts) == 2:\n",
    "        return int(parts[-1])*int(parts[-2])\n",
    "    else:\n",
    "        return -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def get_batches(fn, data_dir, batch_size, shuffle_buffer_size=100000, running_mode='train', args=None,\n",
    "                balance=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      fn: function that tells how to process tfrecord\n",
    "      data_dir: The directory to read data from.\n",
    "      batch_size: The number of elements in a single minibatch.\n",
    "      cycle_length: The number of input elements to process concurrently in the dataset loader. (Default: 1)\n",
    "      shuffle_buffer_size: The number of records to load before shuffling. (Default: 100000)\n",
    "      running_mode: string that is used to determine from where the data should be loaded (Default: train)\n",
    "      args: list of arguments that will be passed into function that processes tfrecord (Default: None)\n",
    "\n",
    "    Returns:\n",
    "      A batch worth of data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading files from {}\".format(data_dir))\n",
    "    filenames = tf.gfile.Glob(os.path.join(data_dir, running_mode, \"*.tfrecords\"))\n",
    "    print(\"Found {} file(s)\".format(len(filenames)))\n",
    "    upsampling_factor = [ get_upsampling_factor(fn) for fn in filenames]\n",
    "    print(upsampling_factor)\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices((filenames, upsampling_factor))\n",
    "    print(filename_dataset)\n",
    "    filename_dataset = filename_dataset.shuffle(len(filenames))\n",
    "    prefetch = max(int(batch_size / len(filenames)), 1)\n",
    "\n",
    "    # Repeat data in the file for unlimited number. This solves class imbalance problem.\n",
    "    def get_tfrecord_dataset(filename, upsampling_factor):\n",
    "        tfrecord_dataset = tf.data.TFRecordDataset(filename).prefetch(prefetch)\n",
    "        if balance:\n",
    "            tfrecord_dataset = tfrecord_dataset.repeat(tf.cast(upsampling_factor, dtype=tf.int64))\n",
    "        return tfrecord_dataset\n",
    "\n",
    "    dataset = filename_dataset.interleave(\n",
    "        lambda filename, upsampling_factor: get_tfrecord_dataset(filename, upsampling_factor),\n",
    "        cycle_length=len(filenames))\n",
    "    print(\"Loading process will use {} CPUs\".format(multiprocessing.cpu_count()))\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(lambda x: fn(x, args), num_parallel_calls=multiprocessing.cpu_count()).prefetch(batch_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def get_batch(batch_size, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding):\n",
    "    path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep))\n",
    "    extract_fn = extract_seq_and_label\n",
    "    batches = get_batches(extract_fn, path, batch_size, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                running_mode=running_mode, args=[[512], dynamic_padding])\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "def prepare_real_data(real_x, labels, data_dir, dataset):\n",
    "    real_x = tf.reshape(real_x, [1, 512], name=REAL_PROTEINS)\n",
    "    #reactions = get_reactions(labels, data_dir, dataset)\n",
    "    labels = tf.identity(tf.squeeze(labels), name=LABELS)\n",
    "    real_x = convert_real_to_one_hot(real_x)\n",
    "    real_x = tf.expand_dims(real_x, 1)\n",
    "    return real_x, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def get_reactions(labels, data_dir, dataset):\n",
    "    reaction_tensors = []\n",
    "    for reaction in get_reaction_data(data_dir, dataset):\n",
    "        # label = tf.convert_to_tensor(reaction[0])\n",
    "        r = [pad_up_to(tf.convert_to_tensor(reaction[i]), [128], True) for\n",
    "             i in range(1, 5)]\n",
    "        reaction_tensors.append(r)\n",
    "    reaction_tensors = tf.stack(reaction_tensors)\n",
    "    return tf.gather_nd(reaction_tensors, tf.expand_dims(labels, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def convert_real_to_one_hot(real_x): # Label smoothing?\n",
    "    real_to_display = tf.one_hot(real_x, 21, axis=1)\n",
    "    real_to_display = tf.transpose(real_to_display, perm=[0, 2, 1])\n",
    "    return real_to_display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def get_reaction_data(data_dir, dataset):\n",
    "    filename = \"train\" + \"_reactions.npy\"\n",
    "    reaction_path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep), filename)\n",
    "    try:\n",
    "        reactions = np.load(reaction_path)\n",
    "    except Exception as e:\n",
    "        tf.logging.warn(\"Reaction file could not be loaded: \" + e.__str__())\n",
    "        reactions = []\n",
    "    return reactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = \"/home/evgeny/code/AAVGAN/src/gan\"\n",
    "DATASET = 'protein'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from /home/evgeny/code/AAVGAN/src/gan/../../data/protein/capsids\n",
      "Found 1 file(s)\n",
      "[1]\n",
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int32)>\n",
      "Loading process will use 12 CPUs\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  5  3  9 12  4  3 13  9  3 18 14  9 16 10  6\n",
      "  19  5 18  9 15 10 12  9  3 10  1 18  8 20 16 12  7  7  2  3  8 14  3  8\n",
      "   9  3 13  4  3 15  1  9 12 10  9 18 19 18  4  3  6 13 17  9 13 20  9 20\n",
      "   5  7 15 14 17  9 14  3 20 12  9 13 18 12 10 15  3 20 17  2  8 20 10  5\n",
      "  12  9  3  9  8 17 17  3  6 11  3  6 20  5  1  1  6 12  6  6  8 18  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "[[8]]\n",
      "[[[[1. 0. 0. ... 0. 0. 0.]\n",
      "   [1. 0. 0. ... 0. 0. 0.]\n",
      "   [1. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [1. 0. 0. ... 0. 0. 0.]\n",
      "   [1. 0. 0. ... 0. 0. 0.]\n",
      "   [1. 0. 0. ... 0. 0. 0.]]]]\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(CURRENT_DIRECTORY, '..\\..\\data'.replace(\"\\\\\", os.sep))\n",
    "dataset = DATASET + \"/capsids\"\n",
    "already_embedded = False\n",
    "shuffle_buffer_size = 100000\n",
    "running_mode = \"train\"\n",
    "dynamic_padding = True\n",
    "sess = tf.InteractiveSession()\n",
    "batch = get_batch(1, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding)\n",
    "real_x, labels = batch[0], batch[1]\n",
    "print(real_x.eval())\n",
    "print(labels.eval())\n",
    "real_x, labels = prepare_real_data(real_x, labels, data_dir, dataset)\n",
    "print(real_x.eval())\n",
    "print(labels.eval())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}