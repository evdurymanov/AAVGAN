{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/evgeny/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from absl import flags\n",
    "import common.model.utils_ori as utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from common.bio.amino_acid import numpy_seqs_to_fasta\n",
    "\n",
    "from common.bio.constants import get_lesk_color_mapping\n",
    "from gan.documentation import add_image_grid\n",
    "from gan.protein.custom_scalars import add_custom_scalar\n",
    "from gan.protein.helpers import convert_to_acid_ids, REAL_PROTEINS, ACID_EMBEDDINGS_SCOPE, ACID_EMBEDDINGS, \\\n",
    "    FAKE_PROTEINS, CLASS_MAPPING, SEQ_LENGTH, get_shape, LABELS, NUM_AMINO_ACIDS, get_file\n",
    "from common.model import ops\n",
    "from common.model.ops import pad_up_to, gelu\n",
    "\n",
    "\n",
    "from gan.parameters import get_flags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_seq_and_label(record, args):\n",
    "    \"\"\"Extracts and preprocesses the sequences and label from the record.\n",
    "\n",
    "    Args:\n",
    "      record: tfrecord from file\n",
    "      args: list of arguments\n",
    "\n",
    "    Returns:\n",
    "      Sequence and label as tensors\n",
    "\n",
    "    \"\"\"\n",
    "    features = tf.parse_single_example(\n",
    "        serialized=record,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([1], tf.int64),\n",
    "            'sequence': tf.FixedLenSequenceFeature([], dtype=tf.int64, allow_missing=True)\n",
    "        },\n",
    "    )\n",
    "    seq = tf.cast(features['sequence'], tf.int32, name=\"seq\")\n",
    "    labels = tf.cast(features['label'], tf.int32, name=\"labels\")\n",
    "\n",
    "    seq = pad_up_to(seq, args[0], dynamic_padding=args[1])\n",
    "    return seq, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_upsampling_factor(full_path):\n",
    "    \"\"\"\n",
    "    Function that parses the file name to know how much upsampling is required.\n",
    "    Args:\n",
    "        full_path: a path of the tfrecords file\n",
    "\n",
    "    Returns:\n",
    "        return upsampling factor. If file is not in the right format return -1 (infinite upsampling)\n",
    "    \"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(full_path))[0]\n",
    "    parts = filename.split(\"_\")\n",
    "    if len(parts) == 3 or len(parts) == 2:\n",
    "        return int(parts[-1])*int(parts[-2])\n",
    "    else:\n",
    "        return -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_batches(fn, data_dir, batch_size, shuffle_buffer_size=100000, running_mode='train', args=None,\n",
    "                balance=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      fn: function that tells how to process tfrecord\n",
    "      data_dir: The directory to read data from.\n",
    "      batch_size: The number of elements in a single minibatch.\n",
    "      cycle_length: The number of input elements to process concurrently in the dataset loader. (Default: 1)\n",
    "      shuffle_buffer_size: The number of records to load before shuffling. (Default: 100000)\n",
    "      running_mode: string that is used to determine from where the data should be loaded (Default: train)\n",
    "      args: list of arguments that will be passed into function that processes tfrecord (Default: None)\n",
    "\n",
    "    Returns:\n",
    "      A batch worth of data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading files from {}\".format(data_dir))\n",
    "    filenames = tf.gfile.Glob(os.path.join(data_dir, running_mode, \"*.tfrecords\"))\n",
    "    print(\"Found {} file(s)\".format(len(filenames)))\n",
    "    upsampling_factor = [ get_upsampling_factor(fn) for fn in filenames]\n",
    "    print(upsampling_factor)\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices((filenames, upsampling_factor))\n",
    "    print(list(filename_dataset))\n",
    "    filename_dataset = filename_dataset.shuffle(len(filenames))\n",
    "    prefetch = max(int(batch_size / len(filenames)), 1)\n",
    "\n",
    "    # Repeat data in the file for unlimited number. This solves class imbalance problem.\n",
    "    def get_tfrecord_dataset(filename, upsampling_factor):\n",
    "        tfrecord_dataset = tf.data.TFRecordDataset(filename).prefetch(prefetch)\n",
    "        if balance:\n",
    "            tfrecord_dataset = tfrecord_dataset.repeat(tf.cast(upsampling_factor, dtype=tf.int64))\n",
    "        return tfrecord_dataset\n",
    "\n",
    "    dataset = filename_dataset.interleave(\n",
    "        lambda filename, upsampling_factor: get_tfrecord_dataset(filename, upsampling_factor),\n",
    "        cycle_length=len(filenames))\n",
    "    print(list(dataset))\n",
    "    print(\"Loading process will use {} CPUs\".format(multiprocessing.cpu_count()))\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(lambda x: fn(x, args), num_parallel_calls=multiprocessing.cpu_count()).prefetch(batch_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_batch(batch_size, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding):\n",
    "    path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep))\n",
    "    extract_fn = extract_seq_and_label\n",
    "    batches = get_batches(extract_fn, path, batch_size, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                running_mode=running_mode, args=[[512], dynamic_padding])\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def prepare_real_data(real_x, labels, data_dir, dataset):\n",
    "    real_x = tf.reshape(real_x, [10, 512], name=REAL_PROTEINS)\n",
    "    #reactions = get_reactions(labels, data_dir, dataset)\n",
    "    labels = tf.identity(tf.squeeze(labels), name=LABELS)\n",
    "    real_x = convert_real_to_one_hot(real_x)\n",
    "    real_x = tf.expand_dims(real_x, 1)\n",
    "    return real_x, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_reactions(labels, data_dir, dataset):\n",
    "    reaction_tensors = []\n",
    "    for reaction in get_reaction_data(data_dir, dataset):\n",
    "        # label = tf.convert_to_tensor(reaction[0])\n",
    "        r = [pad_up_to(tf.convert_to_tensor(reaction[i]), [128], True) for\n",
    "             i in range(1, 5)]\n",
    "        reaction_tensors.append(r)\n",
    "    reaction_tensors = tf.stack(reaction_tensors)\n",
    "    return tf.gather_nd(reaction_tensors, tf.expand_dims(labels, axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def convert_real_to_one_hot(real_x): # Label smoothing?\n",
    "    real_to_display = tf.one_hot(real_x, 21, axis=1)\n",
    "    real_to_display = tf.transpose(real_to_display, perm=[0, 2, 1])\n",
    "    return real_to_display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_reaction_data(data_dir, dataset):\n",
    "    filename = \"train\" + \"_reactions.npy\"\n",
    "    reaction_path = os.path.join(data_dir, dataset.replace(\"\\\\\", os.sep), filename)\n",
    "    try:\n",
    "        reactions = np.load(reaction_path)\n",
    "    except Exception as e:\n",
    "        tf.logging.warn(\"Reaction file could not be loaded: \" + e.__str__())\n",
    "        reactions = []\n",
    "    return reactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = \"/home/evgeny/code/AAVGAN/src/gan\"\n",
    "DATASET = 'protein'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 15:22:43.110176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 15:22:43.110314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:01:00.0\n",
      "2022-06-13 15:22:43.110351: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-06-13 15:22:43.110361: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2022-06-13 15:22:43.110370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2022-06-13 15:22:43.110379: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2022-06-13 15:22:43.110388: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2022-06-13 15:22:43.110397: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2022-06-13 15:22:43.110406: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-06-13 15:22:43.110457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 15:22:43.110611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 15:22:43.110712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2022-06-13 15:22:43.110732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-13 15:22:43.110751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2022-06-13 15:22:43.110755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2022-06-13 15:22:43.110814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 15:22:43.110935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 15:22:43.111030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6963 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files from /home/evgeny/code/AAVGAN/src/gan/../../data/protein/testing_cgan_up_150\n",
      "Found 14 file(s)\n",
      "[30, 90, 50, 450, 20, 5, 1, 2, 80, 40, 3, 10, 70, 60]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_25591/98033955.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdynamic_padding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0msess\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInteractiveSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshuffle_buffer_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrunning_mode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdynamic_padding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mreal_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#print(real_x.eval())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_25591/1615012235.py\u001B[0m in \u001B[0;36mget_batch\u001B[0;34m(batch_size, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding)\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mextract_fn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_seq_and_label\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     batches = get_batches(extract_fn, path, batch_size, shuffle_buffer_size=shuffle_buffer_size,\n\u001B[0;32m----> 5\u001B[0;31m                                 running_mode=running_mode, args=[[512], dynamic_padding])\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mbatches\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_25591/1867387380.py\u001B[0m in \u001B[0;36mget_batches\u001B[0;34m(fn, data_dir, batch_size, shuffle_buffer_size, running_mode, args, balance)\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mupsampling_factor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0mget_upsampling_factor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfilenames\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mupsampling_factor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m     \u001B[0mfilename_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilenames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mupsampling_factor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sas\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0mfilename_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilename_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilenames\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mfrom_tensor_slices\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m   1683\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDatasetV2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1684\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1685\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mDatasetV1Adapter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDatasetV2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1686\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1687\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mfrom_tensor_slices\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    362\u001B[0m       \u001B[0mDataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m     \"\"\"\n\u001B[0;32m--> 364\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mTensorSliceDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m   \u001B[0;32mclass\u001B[0m \u001B[0m_GeneratorState\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, tensors)\u001B[0m\n\u001B[1;32m   2219\u001B[0m     \u001B[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2220\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tensors\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2221\u001B[0;31m       \u001B[0mtensors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2223\u001B[0m     \u001B[0mbatched_structure\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStructure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001B[0m in \u001B[0;36mnormalize_tensors\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    304\u001B[0m         \u001B[0mprepared\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 306\u001B[0;31m         \u001B[0mprepared\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"component_%d\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    307\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpack_sequence_as\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprepared\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor\u001B[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001B[0m\n\u001B[1;32m   1085\u001B[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001B[1;32m   1086\u001B[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001B[0;32m-> 1087\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mconvert_to_tensor_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreferred_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1088\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1089\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor_v2\u001B[0;34m(value, dtype, dtype_hint, name)\u001B[0m\n\u001B[1;32m   1143\u001B[0m       \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1144\u001B[0m       \u001B[0mpreferred_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype_hint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1145\u001B[0;31m       as_ref=False)\n\u001B[0m\u001B[1;32m   1146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1147\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36minternal_convert_to_tensor\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001B[0m\n\u001B[1;32m   1222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1223\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1224\u001B[0;31m       \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconversion_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mas_ref\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1225\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1226\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36m_constant_tensor_conversion_function\u001B[0;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[1;32m    303\u001B[0m                                          as_ref=False):\n\u001B[1;32m    304\u001B[0m   \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 305\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mconstant\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    244\u001B[0m   \"\"\"\n\u001B[1;32m    245\u001B[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001B[0;32m--> 246\u001B[0;31m                         allow_broadcast=True)\n\u001B[0m\u001B[1;32m    247\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    288\u001B[0m       attrs={\"value\": tensor_value,\n\u001B[1;32m    289\u001B[0m              \"dtype\": dtype_value},\n\u001B[0;32m--> 290\u001B[0;31m       name=name).outputs[0]\n\u001B[0m\u001B[1;32m    291\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mconst_tensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001B[0m in \u001B[0;36mnew_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    505\u001B[0m                 \u001B[0;34m'in a future version'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mdate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m'after %s'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mdate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    506\u001B[0m                 instructions)\n\u001B[0;32m--> 507\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    508\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mcreate_op\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3605\u001B[0m     \u001B[0;31m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3606\u001B[0m     \u001B[0;31m# Session.run call cannot occur between creating and mutating the op.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3607\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mutation_lock\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3608\u001B[0m       ret = Operation(\n\u001B[1;32m   3609\u001B[0m           \u001B[0mnode_def\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/util/lock_util.py\u001B[0m in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__enter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_group_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype_arg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue_arg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraceback_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tf1.14/lib/python3.7/site-packages/tensorflow/python/util/lock_util.py\u001B[0m in \u001B[0;36macquire\u001B[0;34m(self, group_id)\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_group_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroup_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ready\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_another_group_active\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroup_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ready\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(CURRENT_DIRECTORY, '..\\..\\data'.replace(\"\\\\\", os.sep))\n",
    "dataset = DATASET + \"/testing_cgan_up_150\"\n",
    "already_embedded = False\n",
    "shuffle_buffer_size = 100000\n",
    "running_mode = \"train\"\n",
    "dynamic_padding = True\n",
    "sess = tf.InteractiveSession()\n",
    "batch = get_batch(10, data_dir, dataset, shuffle_buffer_size, running_mode, dynamic_padding)\n",
    "real_x, labels = batch[0], batch[1]\n",
    "#print(real_x.eval())\n",
    "#print(labels.eval())\n",
    "real_x, labels = prepare_real_data(real_x, labels, data_dir, dataset)\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}